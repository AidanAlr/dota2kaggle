{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import RobustScaler, QuantileTransformer\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif, RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "df = pd.read_csv('train_data.csv')\n",
    "test_features = pd.read_csv('test_data.csv')"
   ],
   "metadata": {
    "id": "ibbWSJrvUxyL",
    "ExecuteTime": {
     "end_time": "2025-06-05T00:29:37.824213Z",
     "start_time": "2025-06-05T00:29:36.745343Z"
    }
   },
   "outputs": [],
   "execution_count": 366
  },
  {
   "cell_type": "code",
   "source": [
    "print(list(df.columns))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pgGNHQSBbHz2",
    "outputId": "892b98fb-909b-4778-a0b1-f8d505d37fad",
    "ExecuteTime": {
     "end_time": "2025-06-05T00:29:37.830220Z",
     "start_time": "2025-06-05T00:29:37.825220Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['game_time', 'game_mode', 'lobby_type', 'objectives_len', 'chat_len', 'r1_hero_id', 'r1_kills', 'r1_deaths', 'r1_assists', 'r1_denies', 'r1_gold', 'r1_lh', 'r1_xp', 'r1_health', 'r1_max_health', 'r1_max_mana', 'r1_level', 'r1_x', 'r1_y', 'r1_stuns', 'r1_creeps_stacked', 'r1_camps_stacked', 'r1_rune_pickups', 'r1_firstblood_claimed', 'r1_teamfight_participation', 'r1_towers_killed', 'r1_roshans_killed', 'r1_obs_placed', 'r1_sen_placed', 'r2_hero_id', 'r2_kills', 'r2_deaths', 'r2_assists', 'r2_denies', 'r2_gold', 'r2_lh', 'r2_xp', 'r2_health', 'r2_max_health', 'r2_max_mana', 'r2_level', 'r2_x', 'r2_y', 'r2_stuns', 'r2_creeps_stacked', 'r2_camps_stacked', 'r2_rune_pickups', 'r2_firstblood_claimed', 'r2_teamfight_participation', 'r2_towers_killed', 'r2_roshans_killed', 'r2_obs_placed', 'r2_sen_placed', 'r3_hero_id', 'r3_kills', 'r3_deaths', 'r3_assists', 'r3_denies', 'r3_gold', 'r3_lh', 'r3_xp', 'r3_health', 'r3_max_health', 'r3_max_mana', 'r3_level', 'r3_x', 'r3_y', 'r3_stuns', 'r3_creeps_stacked', 'r3_camps_stacked', 'r3_rune_pickups', 'r3_firstblood_claimed', 'r3_teamfight_participation', 'r3_towers_killed', 'r3_roshans_killed', 'r3_obs_placed', 'r3_sen_placed', 'r4_hero_id', 'r4_kills', 'r4_deaths', 'r4_assists', 'r4_denies', 'r4_gold', 'r4_lh', 'r4_xp', 'r4_health', 'r4_max_health', 'r4_max_mana', 'r4_level', 'r4_x', 'r4_y', 'r4_stuns', 'r4_creeps_stacked', 'r4_camps_stacked', 'r4_rune_pickups', 'r4_firstblood_claimed', 'r4_teamfight_participation', 'r4_towers_killed', 'r4_roshans_killed', 'r4_obs_placed', 'r4_sen_placed', 'r5_hero_id', 'r5_kills', 'r5_deaths', 'r5_assists', 'r5_denies', 'r5_gold', 'r5_lh', 'r5_xp', 'r5_health', 'r5_max_health', 'r5_max_mana', 'r5_level', 'r5_x', 'r5_y', 'r5_stuns', 'r5_creeps_stacked', 'r5_camps_stacked', 'r5_rune_pickups', 'r5_firstblood_claimed', 'r5_teamfight_participation', 'r5_towers_killed', 'r5_roshans_killed', 'r5_obs_placed', 'r5_sen_placed', 'd1_hero_id', 'd1_kills', 'd1_deaths', 'd1_assists', 'd1_denies', 'd1_gold', 'd1_lh', 'd1_xp', 'd1_health', 'd1_max_health', 'd1_max_mana', 'd1_level', 'd1_x', 'd1_y', 'd1_stuns', 'd1_creeps_stacked', 'd1_camps_stacked', 'd1_rune_pickups', 'd1_firstblood_claimed', 'd1_teamfight_participation', 'd1_towers_killed', 'd1_roshans_killed', 'd1_obs_placed', 'd1_sen_placed', 'd2_hero_id', 'd2_kills', 'd2_deaths', 'd2_assists', 'd2_denies', 'd2_gold', 'd2_lh', 'd2_xp', 'd2_health', 'd2_max_health', 'd2_max_mana', 'd2_level', 'd2_x', 'd2_y', 'd2_stuns', 'd2_creeps_stacked', 'd2_camps_stacked', 'd2_rune_pickups', 'd2_firstblood_claimed', 'd2_teamfight_participation', 'd2_towers_killed', 'd2_roshans_killed', 'd2_obs_placed', 'd2_sen_placed', 'd3_hero_id', 'd3_kills', 'd3_deaths', 'd3_assists', 'd3_denies', 'd3_gold', 'd3_lh', 'd3_xp', 'd3_health', 'd3_max_health', 'd3_max_mana', 'd3_level', 'd3_x', 'd3_y', 'd3_stuns', 'd3_creeps_stacked', 'd3_camps_stacked', 'd3_rune_pickups', 'd3_firstblood_claimed', 'd3_teamfight_participation', 'd3_towers_killed', 'd3_roshans_killed', 'd3_obs_placed', 'd3_sen_placed', 'd4_hero_id', 'd4_kills', 'd4_deaths', 'd4_assists', 'd4_denies', 'd4_gold', 'd4_lh', 'd4_xp', 'd4_health', 'd4_max_health', 'd4_max_mana', 'd4_level', 'd4_x', 'd4_y', 'd4_stuns', 'd4_creeps_stacked', 'd4_camps_stacked', 'd4_rune_pickups', 'd4_firstblood_claimed', 'd4_teamfight_participation', 'd4_towers_killed', 'd4_roshans_killed', 'd4_obs_placed', 'd4_sen_placed', 'd5_hero_id', 'd5_kills', 'd5_deaths', 'd5_assists', 'd5_denies', 'd5_gold', 'd5_lh', 'd5_xp', 'd5_health', 'd5_max_health', 'd5_max_mana', 'd5_level', 'd5_x', 'd5_y', 'd5_stuns', 'd5_creeps_stacked', 'd5_camps_stacked', 'd5_rune_pickups', 'd5_firstblood_claimed', 'd5_teamfight_participation', 'd5_towers_killed', 'd5_roshans_killed', 'd5_obs_placed', 'd5_sen_placed', 'radiant_win', 'ID']\n"
     ]
    }
   ],
   "execution_count": 367
  },
  {
   "cell_type": "code",
   "source": [
    "dataframes = [df, test_features]\n",
    "\n",
    "player_numbers = ['1', '2', '3', '4', '5']\n",
    "stats = ['gold', 'xp']\n",
    "# Add the new features to both the training and test set\n",
    "for dataframe in dataframes:\n",
    "    # Team Stats\n",
    "    for stat in stats:\n",
    "        dataframe[f'dire_team_{stat}'] = dataframe[[f'd{number}_{stat}' for number in player_numbers]].sum(axis=1)\n",
    "        dataframe[f'radiant_team_{stat}'] = dataframe[[f'r{number}_{stat}' for number in player_numbers]].sum(axis=1)\n",
    "        dataframe[f'dire_team_{stat}_lead'] = dataframe[f'dire_team_{stat}'] - dataframe[f'radiant_team_{stat}']\n",
    "\n",
    "    # Player Stats\n",
    "    for number in player_numbers:\n",
    "        for stat in stats:\n",
    "            dataframe[f'd{number}_{stat}_lead'] = dataframe[f'd{number}_{stat}'] - dataframe[f'r{number}_{stat}']\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "aOepAZeSU46b",
    "ExecuteTime": {
     "end_time": "2025-06-05T00:29:37.880731Z",
     "start_time": "2025-06-05T00:29:37.830726Z"
    }
   },
   "outputs": [],
   "execution_count": 368
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T00:29:37.884239Z",
     "start_time": "2025-06-05T00:29:37.882239Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": 368
  },
  {
   "cell_type": "code",
   "source": [
    "# Remove unwanted columns\n",
    "columns_to_drop = []\n",
    "stats_to_drop = ['gold', 'xp', 'x', 'y']\n",
    "for stat in stats_to_drop:\n",
    "    dire_columns = [f'd{number}_{stat}' for number in player_numbers]\n",
    "    radiant_columns = [f'r{number}_{stat}' for number in player_numbers]\n",
    "    columns_to_drop.extend(dire_columns)\n",
    "    columns_to_drop.extend(radiant_columns)\n",
    "\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "test_features = test_features.drop(columns=columns_to_drop)"
   ],
   "metadata": {
    "id": "c7JUluRGdoLI",
    "ExecuteTime": {
     "end_time": "2025-06-05T00:29:37.911747Z",
     "start_time": "2025-06-05T00:29:37.884742Z"
    }
   },
   "outputs": [],
   "execution_count": 369
  },
  {
   "cell_type": "code",
   "source": [
    "df"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "3J78_5B3c-G8",
    "outputId": "bef91d2a-c408-4834-e554-7fa9a2973ce6",
    "ExecuteTime": {
     "end_time": "2025-06-05T00:29:37.941107Z",
     "start_time": "2025-06-05T00:29:37.912252Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       game_time  game_mode  lobby_type  objectives_len  chat_len  r1_hero_id  \\\n",
       "0            871         22           0               4         2         110   \n",
       "1           2549         22           0              17         0         114   \n",
       "2           1841         22           0               8         1         100   \n",
       "3           2211         22           7              11         3          32   \n",
       "4            458         22           7               1         0          68   \n",
       "...          ...        ...         ...             ...       ...         ...   \n",
       "29670       1664          3           0               8         0          17   \n",
       "29671       2898         22           7              26       108          89   \n",
       "29672       1246         23           0               6        10          51   \n",
       "29673       2620         22           7              15        10         114   \n",
       "29674       1111         22           7               5        12          63   \n",
       "\n",
       "       r1_kills  r1_deaths  r1_assists  r1_denies  ...  d1_gold_lead  \\\n",
       "0             2          3          11          3  ...           827   \n",
       "1            16          2          12         24  ...        -11699   \n",
       "2             2         11          12          2  ...          2334   \n",
       "3            14          3          11         21  ...          2590   \n",
       "4             3          0           0         15  ...          -889   \n",
       "...         ...        ...         ...        ...  ...           ...   \n",
       "29670         1          6           1          8  ...         -2489   \n",
       "29671         4          5          17          5  ...          6123   \n",
       "29672         9          7          15         16  ...          1896   \n",
       "29673         9          8           5         10  ...        -11736   \n",
       "29674         0          2           3          3  ...          -485   \n",
       "\n",
       "       d1_xp_lead  d2_gold_lead  d2_xp_lead  d3_gold_lead  d3_xp_lead  \\\n",
       "0            2134         -2839       -2546          -926        1477   \n",
       "1           -7029          2450       -3167         -9469       -8278   \n",
       "2             105         -4594       -6039          7324        3934   \n",
       "3            4190         -6364      -11050         -2387       -1702   \n",
       "4           -1590           775        1210          -293        -540   \n",
       "...           ...           ...         ...           ...         ...   \n",
       "29670       -4469          8831        6826         -2505       -4606   \n",
       "29671          15         -4808         730         -8490       -6101   \n",
       "29672        -481          1149        2052         -6269       -9874   \n",
       "29673      -11832          5117        9723          7929        7343   \n",
       "29674       -1425           -86        1004           -28         863   \n",
       "\n",
       "       d4_gold_lead  d4_xp_lead  d5_gold_lead  d5_xp_lead  \n",
       "0             -2097       -2655         -3350       -4142  \n",
       "1              4935        -322          2797        -253  \n",
       "2             -1281       -5461         -2107       -1747  \n",
       "3             -7443      -11605          1459        -818  \n",
       "4              -165        1008          -166          24  \n",
       "...             ...         ...           ...         ...  \n",
       "29670          7301        4098          3605        2744  \n",
       "29671          -672         451         -7952       -1388  \n",
       "29672         -2114       -1512          3896        5710  \n",
       "29673         12332         532         -7072          -7  \n",
       "29674          -776       -2392           929        -275  \n",
       "\n",
       "[29675 rows x 223 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>game_time</th>\n",
       "      <th>game_mode</th>\n",
       "      <th>lobby_type</th>\n",
       "      <th>objectives_len</th>\n",
       "      <th>chat_len</th>\n",
       "      <th>r1_hero_id</th>\n",
       "      <th>r1_kills</th>\n",
       "      <th>r1_deaths</th>\n",
       "      <th>r1_assists</th>\n",
       "      <th>r1_denies</th>\n",
       "      <th>...</th>\n",
       "      <th>d1_gold_lead</th>\n",
       "      <th>d1_xp_lead</th>\n",
       "      <th>d2_gold_lead</th>\n",
       "      <th>d2_xp_lead</th>\n",
       "      <th>d3_gold_lead</th>\n",
       "      <th>d3_xp_lead</th>\n",
       "      <th>d4_gold_lead</th>\n",
       "      <th>d4_xp_lead</th>\n",
       "      <th>d5_gold_lead</th>\n",
       "      <th>d5_xp_lead</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>871</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>110</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>827</td>\n",
       "      <td>2134</td>\n",
       "      <td>-2839</td>\n",
       "      <td>-2546</td>\n",
       "      <td>-926</td>\n",
       "      <td>1477</td>\n",
       "      <td>-2097</td>\n",
       "      <td>-2655</td>\n",
       "      <td>-3350</td>\n",
       "      <td>-4142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2549</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>114</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>24</td>\n",
       "      <td>...</td>\n",
       "      <td>-11699</td>\n",
       "      <td>-7029</td>\n",
       "      <td>2450</td>\n",
       "      <td>-3167</td>\n",
       "      <td>-9469</td>\n",
       "      <td>-8278</td>\n",
       "      <td>4935</td>\n",
       "      <td>-322</td>\n",
       "      <td>2797</td>\n",
       "      <td>-253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1841</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2334</td>\n",
       "      <td>105</td>\n",
       "      <td>-4594</td>\n",
       "      <td>-6039</td>\n",
       "      <td>7324</td>\n",
       "      <td>3934</td>\n",
       "      <td>-1281</td>\n",
       "      <td>-5461</td>\n",
       "      <td>-2107</td>\n",
       "      <td>-1747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2211</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>21</td>\n",
       "      <td>...</td>\n",
       "      <td>2590</td>\n",
       "      <td>4190</td>\n",
       "      <td>-6364</td>\n",
       "      <td>-11050</td>\n",
       "      <td>-2387</td>\n",
       "      <td>-1702</td>\n",
       "      <td>-7443</td>\n",
       "      <td>-11605</td>\n",
       "      <td>1459</td>\n",
       "      <td>-818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>458</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>-889</td>\n",
       "      <td>-1590</td>\n",
       "      <td>775</td>\n",
       "      <td>1210</td>\n",
       "      <td>-293</td>\n",
       "      <td>-540</td>\n",
       "      <td>-165</td>\n",
       "      <td>1008</td>\n",
       "      <td>-166</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29670</th>\n",
       "      <td>1664</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>-2489</td>\n",
       "      <td>-4469</td>\n",
       "      <td>8831</td>\n",
       "      <td>6826</td>\n",
       "      <td>-2505</td>\n",
       "      <td>-4606</td>\n",
       "      <td>7301</td>\n",
       "      <td>4098</td>\n",
       "      <td>3605</td>\n",
       "      <td>2744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29671</th>\n",
       "      <td>2898</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>26</td>\n",
       "      <td>108</td>\n",
       "      <td>89</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>6123</td>\n",
       "      <td>15</td>\n",
       "      <td>-4808</td>\n",
       "      <td>730</td>\n",
       "      <td>-8490</td>\n",
       "      <td>-6101</td>\n",
       "      <td>-672</td>\n",
       "      <td>451</td>\n",
       "      <td>-7952</td>\n",
       "      <td>-1388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29672</th>\n",
       "      <td>1246</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>51</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>1896</td>\n",
       "      <td>-481</td>\n",
       "      <td>1149</td>\n",
       "      <td>2052</td>\n",
       "      <td>-6269</td>\n",
       "      <td>-9874</td>\n",
       "      <td>-2114</td>\n",
       "      <td>-1512</td>\n",
       "      <td>3896</td>\n",
       "      <td>5710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29673</th>\n",
       "      <td>2620</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>114</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>-11736</td>\n",
       "      <td>-11832</td>\n",
       "      <td>5117</td>\n",
       "      <td>9723</td>\n",
       "      <td>7929</td>\n",
       "      <td>7343</td>\n",
       "      <td>12332</td>\n",
       "      <td>532</td>\n",
       "      <td>-7072</td>\n",
       "      <td>-7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29674</th>\n",
       "      <td>1111</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>-485</td>\n",
       "      <td>-1425</td>\n",
       "      <td>-86</td>\n",
       "      <td>1004</td>\n",
       "      <td>-28</td>\n",
       "      <td>863</td>\n",
       "      <td>-776</td>\n",
       "      <td>-2392</td>\n",
       "      <td>929</td>\n",
       "      <td>-275</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29675 rows Ã— 223 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 370
  },
  {
   "cell_type": "code",
   "source": [
    "X_train, X_val = train_test_split(df, test_size=0.2)\n",
    "\n",
    "y_train = X_train['radiant_win']\n",
    "X_train = X_train.drop(['radiant_win', 'ID'], axis=1)\n",
    "\n",
    "y_val = X_val['radiant_win']\n",
    "X_val = X_val.drop(['radiant_win', 'ID'], axis=1)\n",
    "\n",
    "X_test = test_features.drop('ID', axis=1)"
   ],
   "metadata": {
    "id": "dNni4MC9U5xm",
    "ExecuteTime": {
     "end_time": "2025-06-05T00:29:38.039112Z",
     "start_time": "2025-06-05T00:29:37.942113Z"
    }
   },
   "outputs": [],
   "execution_count": 371
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DH2vc07KDE7y",
    "ExecuteTime": {
     "end_time": "2025-06-05T00:29:38.045007Z",
     "start_time": "2025-06-05T00:29:38.039616Z"
    }
   },
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, Normalizer\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def plot_scaler_results(scaler_results_dict):\n",
    "    \"\"\"\n",
    "    Plot ROC AUC vs number of features for each scaler\n",
    "    scaler_results_dict: dictionary with scaler names as keys and their feature_test_results as values\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    scaler_names = list(scaler_results_dict.keys())\n",
    "\n",
    "    for i, (scaler_name, results) in enumerate(scaler_results_dict.items()):\n",
    "        ax = axes[i]\n",
    "\n",
    "        # Extract data for plotting\n",
    "        n_features = list(results.keys())\n",
    "        roc_scores = list(results.values())\n",
    "\n",
    "        # Create the plot\n",
    "        ax.plot(n_features, roc_scores, 'b-o', linewidth=2, markersize=4)\n",
    "        ax.set_title(f'{scaler_name}', fontsize=14, fontweight='bold')\n",
    "        ax.set_xlabel('Number of features selected', fontsize=12)\n",
    "        ax.set_ylabel('ROC AUC', fontsize=12)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "        # Find and highlight best point\n",
    "        best_idx = roc_scores.index(max(roc_scores))\n",
    "        ax.plot(n_features[best_idx], roc_scores[best_idx], 'ro', markersize=8,\n",
    "                label=f'Best: {max(roc_scores):.4f}')\n",
    "        ax.legend()\n",
    "\n",
    "        # Set consistent y-axis limits for comparison\n",
    "        ax.set_ylim([min(min(results.values()) for results in scaler_results_dict.values()) - 0.001,\n",
    "                     max(max(results.values()) for results in scaler_results_dict.values()) + 0.001])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('ROC AUC vs Number of Features by Scaler', fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.show()\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 372
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T00:29:38.055012Z",
     "start_time": "2025-06-05T00:29:38.046013Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def fast_feature_selection_pipeline(X_train, y_train, X_val, y_val, X_test, feature_names):\n",
    "    print(\"=== Fast Feature Selection Pipeline ===\")\n",
    "\n",
    "    # Convert DataFrames to numpy arrays for consistent indexing\n",
    "    X_train_np = X_train.values if hasattr(X_train, 'values') else X_train\n",
    "    X_val_np = X_val.values if hasattr(X_val, 'values') else X_val\n",
    "    X_test_np = X_test.values if hasattr(X_test, 'values') else X_test\n",
    "\n",
    "    # Ensure feature_names is a numpy array\n",
    "    if hasattr(feature_names, 'values'):\n",
    "        feature_names_array = feature_names.values\n",
    "    elif hasattr(feature_names, '__iter__') and not isinstance(feature_names, str):\n",
    "        feature_names_array = np.array(list(feature_names))\n",
    "    else:\n",
    "        feature_names_array = np.array(feature_names)\n",
    "\n",
    "    # Test different scalers\n",
    "    scalers = {\n",
    "        'None': None,\n",
    "        'StandardScaler': StandardScaler(),\n",
    "        'RobustScaler': RobustScaler(),\n",
    "        'MinMaxScaler': MinMaxScaler()\n",
    "    }\n",
    "\n",
    "    best_scaler_name = None\n",
    "    best_scaler = None\n",
    "    best_overall_score = 0\n",
    "    best_scaler_results = {}\n",
    "    all_scaler_results = {}  # Store results for all scalers for plotting\n",
    "\n",
    "    for scaler_name, scaler in scalers.items():\n",
    "        print(f\"\\n=== Testing {scaler_name} ===\")\n",
    "\n",
    "        # Apply scaling\n",
    "        if scaler is None:\n",
    "            X_train_scaled = X_train_np.copy()\n",
    "            X_val_scaled = X_val_np.copy()\n",
    "            X_test_scaled = X_test_np.copy()\n",
    "        else:\n",
    "            X_train_scaled = scaler.fit_transform(X_train_np)\n",
    "            X_val_scaled = scaler.transform(X_val_np)\n",
    "            X_test_scaled = scaler.transform(X_test_np)\n",
    "\n",
    "        # Step 2: Get feature importance\n",
    "        print(\"Getting feature importance from Random Forest...\")\n",
    "        rf_importance = RandomForestClassifier(n_estimators=300, n_jobs=-1, random_state=42)\n",
    "        rf_importance.fit(X_train_scaled, y_train)\n",
    "        feature_importance = rf_importance.feature_importances_\n",
    "\n",
    "        # Step 3: Test different numbers of top features\n",
    "        print(\"Testing different numbers of top features...\")\n",
    "        results = {}\n",
    "        \n",
    "        \n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=500,\n",
    "            n_jobs=-1,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        max_number_of_features = len(feature_names_array)  # Use the total number of features\n",
    "        for n_features in range(10, max_number_of_features, 10):\n",
    "            # Select top N features\n",
    "            top_features_idx = np.argsort(feature_importance)[-n_features:]\n",
    "            X_train_test = X_train_scaled[:, top_features_idx]\n",
    "            X_val_test = X_val_scaled[:, top_features_idx]\n",
    "\n",
    "            # Train model\n",
    "            model.fit(X_train_test, y_train)\n",
    "\n",
    "            # Evaluate using the probability predictions\n",
    "            y_val_pred_proba = model.predict_proba(X_val_test)[:, 1]\n",
    "            roc_auc = roc_auc_score(y_val, y_val_pred_proba)\n",
    "            results[n_features] = roc_auc\n",
    "            print(f'Number of features: {n_features}, ROC AUC: {roc_auc:.4f}')\n",
    "\n",
    "        # Store results for plotting\n",
    "        all_scaler_results[scaler_name] = results\n",
    "\n",
    "        # Find best for this scaler\n",
    "        best_n_features_scaler = max(results, key=results.get)\n",
    "        best_score_scaler = results[best_n_features_scaler]\n",
    "        print(f'Best for {scaler_name}: {best_score_scaler:.4f} with {best_n_features_scaler} features')\n",
    "\n",
    "        # Track overall best\n",
    "        if best_score_scaler > best_overall_score:\n",
    "            best_overall_score = best_score_scaler\n",
    "            best_scaler_name = scaler_name\n",
    "            best_scaler = scaler\n",
    "            best_scaler_results = {\n",
    "                'feature_importance': feature_importance,\n",
    "                'best_n_features': best_n_features_scaler,\n",
    "                'feature_test_results': results,\n",
    "                'X_train_scaled': X_train_scaled,\n",
    "                'X_val_scaled': X_val_scaled,\n",
    "                'X_test_scaled': X_test_scaled\n",
    "            }\n",
    "\n",
    "    # Plot results for all scalers (uncomment if you have this function)\n",
    "    plot_scaler_results(all_scaler_results)\n",
    "\n",
    "    print(f\"\\n=== BEST OVERALL: {best_scaler_name} with score {best_overall_score:.4f} ===\")\n",
    "\n",
    "    # Step 4: Train final model with best scaler and best number of features\n",
    "    top_features_idx = np.argsort(best_scaler_results['feature_importance'])[-best_scaler_results['best_n_features']:]\n",
    "    X_train_final = best_scaler_results['X_train_scaled'][:, top_features_idx]\n",
    "    X_val_final = best_scaler_results['X_val_scaled'][:, top_features_idx]\n",
    "    X_test_final = best_scaler_results['X_test_scaled'][:, top_features_idx]\n",
    "    final_feature_names = feature_names_array[top_features_idx]\n",
    "\n",
    "    final_model = model\n",
    "    final_model.fit(X_train_final, y_train)\n",
    "\n",
    "    # Final evaluation\n",
    "    y_val_pred_proba = final_model.predict_proba(X_val_final)[:, 1]\n",
    "    final_roc_auc = roc_auc_score(y_val, y_val_pred_proba)\n",
    "\n",
    "    print(f'Final ROC AUC: {final_roc_auc:.4f}')\n",
    "\n",
    "    return {\n",
    "        'best_scaler_name': best_scaler_name,\n",
    "        'best_scaler': best_scaler,\n",
    "        'feature_indices': top_features_idx,\n",
    "        'final_model': final_model,\n",
    "        'feature_names': final_feature_names,\n",
    "        'val_roc_auc': final_roc_auc,\n",
    "        'best_n_features': best_scaler_results['best_n_features'],\n",
    "        'feature_test_results': best_scaler_results['feature_test_results'],\n",
    "        'all_scaler_results': all_scaler_results,\n",
    "        'test_predictions': final_model.predict_proba(X_test_final)[:, 1]\n",
    "    }"
   ],
   "outputs": [],
   "execution_count": 373
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T00:29:38.058861Z",
     "start_time": "2025-06-05T00:29:38.056522Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# results = fast_feature_selection_pipeline(\n",
    "#     X_train,\n",
    "#     y_train,\n",
    "#     X_val,\n",
    "#     y_val,\n",
    "#     X_test,\n",
    "#     X_train.columns,\n",
    "# )\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 374
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T00:29:38.080320Z",
     "start_time": "2025-06-05T00:29:38.059866Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "\n",
    "from sklearn.experimental import enable_halving_search_cv  # noqa\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "def tune_rf_with_gridsearch(model, X_train, y_train, cv_folds=5, scoring='roc_auc'):\n",
    "    \"\"\"\n",
    "    Tune Random Forest hyperparameters using GridSearchCV\n",
    "    \"\"\"\n",
    "    print(\"=== Grid Search Hyperparameter Tuning ===\")\n",
    "    param_grid = {\n",
    "        'n_estimators': [300, 400, 500],          \n",
    "        'max_depth': [10, 15, 20],               \n",
    "    }\n",
    "    # Grid search\n",
    "    grid_search = HalvingGridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        scoring=scoring,\n",
    "        n_jobs=-1,\n",
    "        verbose=1,\n",
    "    )\n",
    "    start_time = time.time()\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f\"Grid Search completed in {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"Best CV Score: {grid_search.best_score_:.4f}\")\n",
    "    print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "    return grid_search\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from typing import Dict, Tuple, Any, Optional, Union\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class FeatureSelectionResult:\n",
    "    \"\"\"Data class to hold feature selection results\"\"\"\n",
    "    best_scaler_name: str\n",
    "    best_scaler: Any\n",
    "    feature_indices: np.ndarray\n",
    "    final_model: RandomForestClassifier\n",
    "    feature_names: np.ndarray\n",
    "    val_roc_auc: float\n",
    "    best_n_features: int\n",
    "    best_params: Dict\n",
    "    test_predictions: np.ndarray\n",
    "\n",
    "class OptimizedFeatureSelector:\n",
    "    \"\"\"\n",
    "    High-performance feature selection pipeline optimized for ROC AUC\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cv_folds: int = 5, n_jobs: int = -1, random_state: int = 42):\n",
    "        self.cv_folds = cv_folds\n",
    "        self.n_jobs = n_jobs\n",
    "        self.random_state = random_state\n",
    "\n",
    "        # Optimized hyperparameter grid for ROC AUC\n",
    "        self.rf_param_grid = {\n",
    "            'n_estimators': [200, 400],\n",
    "            'max_depth': [10, 15, None],\n",
    "            'min_samples_split': [5, 10],\n",
    "            'min_samples_leaf': [2, 4],\n",
    "            'class_weight': ['balanced', None]\n",
    "        }\n",
    "\n",
    "        # Scalers to test\n",
    "        self.scalers = {\n",
    "            'StandardScaler': StandardScaler(),\n",
    "            'RobustScaler': RobustScaler(),\n",
    "            'MinMaxScaler': MinMaxScaler(),\n",
    "            'None': None\n",
    "        }\n",
    "\n",
    "    def _validate_inputs(self, X_train, y_train, X_val, y_val, X_test, feature_names):\n",
    "        \"\"\"Validate and convert inputs to consistent format\"\"\"\n",
    "        # Convert to numpy arrays\n",
    "        X_train = self._to_numpy(X_train)\n",
    "        X_val = self._to_numpy(X_val)\n",
    "        X_test = self._to_numpy(X_test)\n",
    "        y_train = self._to_numpy(y_train).ravel()\n",
    "        y_val = self._to_numpy(y_val).ravel()\n",
    "\n",
    "        # Validate feature names\n",
    "        feature_names = self._validate_feature_names(feature_names, X_train.shape[1])\n",
    "\n",
    "        # Basic validation\n",
    "        assert X_train.shape[1] == X_val.shape[1] == X_test.shape[1], \"Feature dimension mismatch\"\n",
    "        assert len(feature_names) == X_train.shape[1], \"Feature names length mismatch\"\n",
    "\n",
    "        return X_train, y_train, X_val, y_val, X_test, feature_names\n",
    "\n",
    "    def _to_numpy(self, data):\n",
    "        \"\"\"Convert data to numpy array\"\"\"\n",
    "        if hasattr(data, 'values'):\n",
    "            return data.values\n",
    "        return np.array(data) if not isinstance(data, np.ndarray) else data\n",
    "\n",
    "    def _validate_feature_names(self, feature_names, n_features):\n",
    "        \"\"\"Validate and convert feature names\"\"\"\n",
    "        if feature_names is None:\n",
    "            return np.array([f'feature_{i}' for i in range(n_features)])\n",
    "\n",
    "        if hasattr(feature_names, 'values'):\n",
    "            return feature_names.values\n",
    "        elif hasattr(feature_names, '__iter__') and not isinstance(feature_names, str):\n",
    "            return np.array(list(feature_names))\n",
    "        else:\n",
    "            return np.array(feature_names)\n",
    "\n",
    "    def _apply_scaling(self, scaler, X_train, X_val, X_test):\n",
    "        \"\"\"Apply scaling transformation\"\"\"\n",
    "        if scaler is None:\n",
    "            return X_train.copy(), X_val.copy(), X_test.copy()\n",
    "\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_val_scaled = scaler.transform(X_val)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        return X_train_scaled, X_val_scaled, X_test_scaled\n",
    "\n",
    "    def _get_feature_importance(self, X_train, y_train):\n",
    "        \"\"\"Get feature importance using Random Forest\"\"\"\n",
    "        rf = RandomForestClassifier(\n",
    "            n_estimators=100,  # Reduced for speed in importance calculation\n",
    "            n_jobs=self.n_jobs,\n",
    "            random_state=self.random_state,\n",
    "            class_weight='balanced'\n",
    "        )\n",
    "        rf.fit(X_train, y_train)\n",
    "        return rf.feature_importances_\n",
    "\n",
    "    def _optimize_hyperparameters(self, X_train, y_train):\n",
    "        \"\"\"Optimize hyperparameters using GridSearchCV\"\"\"\n",
    "        rf = RandomForestClassifier(random_state=self.random_state, n_jobs=1)  # n_jobs=1 for GridSearch parallelization\n",
    "        grid_search = HalvingGridSearchCV(\n",
    "            estimator=rf,\n",
    "            param_grid=self.rf_param_grid,\n",
    "            scoring='roc_auc',\n",
    "            n_jobs=self.n_jobs,\n",
    "            return_train_score=False\n",
    "        )\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        return grid_search\n",
    "\n",
    "    def _test_feature_counts(self, X_train, X_val, y_train, y_val, feature_importance, max_features):\n",
    "        \"\"\"Test different numbers of features efficiently\"\"\"\n",
    "        results = {}\n",
    "\n",
    "        # Smart feature range selection\n",
    "        min_features = round(0.5 * max_features)\n",
    "        feature_steps = 5\n",
    "        feature_range = range(min_features, max_features + 1, feature_steps)\n",
    "\n",
    "        # Ensure we test the full feature set\n",
    "        if max_features not in feature_range:\n",
    "            feature_range = list(feature_range) + [max_features]\n",
    "\n",
    "        for n_features in feature_range:\n",
    "            logger.info(f\"Testing {n_features} features...\")\n",
    "\n",
    "            try:\n",
    "                # Select top N features\n",
    "                top_features_idx = np.argsort(feature_importance)[-n_features:]\n",
    "                X_train_subset = X_train[:, top_features_idx]\n",
    "                X_val_subset = X_val[:, top_features_idx]\n",
    "\n",
    "                # Optimize hyperparameters\n",
    "                grid_search = self._optimize_hyperparameters(X_train_subset, y_train)\n",
    "\n",
    "                # Evaluate on validation set\n",
    "                best_model = grid_search.best_estimator_\n",
    "                y_val_pred_proba = best_model.predict_proba(X_val_subset)[:, 1]\n",
    "                roc_auc = roc_auc_score(y_val, y_val_pred_proba)\n",
    "\n",
    "                results[n_features] = {\n",
    "                    'roc_auc': roc_auc,\n",
    "                    'best_params': grid_search.best_params_,\n",
    "                    'cv_score': grid_search.best_score_,\n",
    "                    'feature_indices': top_features_idx\n",
    "                }\n",
    "\n",
    "                logger.info(f\"Features: {n_features}, Val ROC AUC: {roc_auc:.4f}, CV Score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error testing {n_features} features: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _evaluate_scaler(self, scaler_name, scaler, X_train, X_val, X_test, y_train, y_val, feature_names):\n",
    "        \"\"\"Evaluate a single scaler configuration\"\"\"\n",
    "        logger.info(f\"Testing {scaler_name}...\")\n",
    "\n",
    "        # Apply scaling\n",
    "        X_train_scaled, X_val_scaled, X_test_scaled = self._apply_scaling(\n",
    "            scaler, X_train, X_val, X_test\n",
    "        )\n",
    "\n",
    "        # Get feature importance\n",
    "        feature_importance = self._get_feature_importance(X_train_scaled, y_train)\n",
    "\n",
    "        # Test different feature counts\n",
    "        results = self._test_feature_counts(\n",
    "            X_train_scaled, X_val_scaled, y_train, y_val,\n",
    "            feature_importance, len(feature_names)\n",
    "        )\n",
    "\n",
    "        if not results:\n",
    "            return None\n",
    "\n",
    "        # Find best configuration for this scaler\n",
    "        best_n_features = max(results, key=lambda x: results[x]['roc_auc'])\n",
    "        best_result = results[best_n_features]\n",
    "\n",
    "        logger.info(f\"Best for {scaler_name}: {best_result['roc_auc']:.4f} with {best_n_features} features\")\n",
    "\n",
    "        return {\n",
    "            'scaler_name': scaler_name,\n",
    "            'scaler': scaler,\n",
    "            'best_n_features': best_n_features,\n",
    "            'best_result': best_result,\n",
    "            'feature_importance': feature_importance,\n",
    "            'scaled_data': (X_train_scaled, X_val_scaled, X_test_scaled)\n",
    "        }\n",
    "\n",
    "    def fit(self, X_train, y_train, X_val, y_val, X_test, feature_names=None) -> FeatureSelectionResult:\n",
    "        \"\"\"\n",
    "        Main method to perform optimized feature selection\n",
    "        \n",
    "        Args:\n",
    "            X_train, y_train: Training data\n",
    "            X_val, y_val: Validation data\n",
    "            X_test: Test data\n",
    "            feature_names: Feature names (optional)\n",
    "            \n",
    "        Returns:\n",
    "            FeatureSelectionResult with optimized model and features\n",
    "        \"\"\"\n",
    "        logger.info(\"Starting optimized feature selection pipeline...\")\n",
    "\n",
    "        # Validate inputs\n",
    "        X_train, y_train, X_val, y_val, X_test, feature_names = self._validate_inputs(\n",
    "            X_train, y_train, X_val, y_val, X_test, feature_names\n",
    "        )\n",
    "\n",
    "        best_overall_score = 0\n",
    "        best_config = None\n",
    "\n",
    "        # Test each scaler\n",
    "        for scaler_name, scaler in self.scalers.items():\n",
    "            config = self._evaluate_scaler(\n",
    "                scaler_name, scaler, X_train, X_val, X_test,\n",
    "                y_train, y_val, feature_names\n",
    "            )\n",
    "\n",
    "            if config and config['best_result']['roc_auc'] > best_overall_score:\n",
    "                best_overall_score = config['best_result']['roc_auc']\n",
    "                best_config = config\n",
    "\n",
    "        if best_config is None:\n",
    "            raise ValueError(\"No successful configuration found!\")\n",
    "\n",
    "        logger.info(f\"Best overall: {best_config['scaler_name']} with ROC AUC {best_overall_score:.4f}\")\n",
    "\n",
    "        # Train final model\n",
    "        return self._train_final_model(best_config, y_train, y_val, feature_names)\n",
    "\n",
    "    def _train_final_model(self, best_config, y_train, y_val, feature_names) -> FeatureSelectionResult:\n",
    "        \"\"\"Train the final model with best configuration\"\"\"\n",
    "        # Get scaled data and feature indices\n",
    "        X_train_scaled, X_val_scaled, X_test_scaled = best_config['scaled_data']\n",
    "        feature_indices = best_config['best_result']['feature_indices']\n",
    "\n",
    "        # Select final features\n",
    "        X_train_final = X_train_scaled[:, feature_indices]\n",
    "        X_val_final = X_val_scaled[:, feature_indices]\n",
    "        X_test_final = X_test_scaled[:, feature_indices]\n",
    "        final_feature_names = feature_names[feature_indices]\n",
    "\n",
    "        # Train final model\n",
    "        final_model = RandomForestClassifier(\n",
    "            **best_config['best_result']['best_params'],\n",
    "            random_state=self.random_state,\n",
    "            n_jobs=self.n_jobs\n",
    "        )\n",
    "        final_model.fit(X_train_final, y_train)\n",
    "\n",
    "        # Final evaluation\n",
    "        y_val_pred_proba = final_model.predict_proba(X_val_final)[:, 1]\n",
    "        final_roc_auc = roc_auc_score(y_val, y_val_pred_proba)\n",
    "\n",
    "        # Generate test predictions\n",
    "        test_predictions = final_model.predict_proba(X_test_final)[:, 1]\n",
    "\n",
    "        logger.info(f\"Final model ROC AUC: {final_roc_auc:.4f}\")\n",
    "        logger.info(f\"Selected {len(feature_indices)} features: {final_feature_names[:5]}...\")\n",
    "\n",
    "        return FeatureSelectionResult(\n",
    "            best_scaler_name=best_config['scaler_name'],\n",
    "            best_scaler=best_config['scaler'],\n",
    "            feature_indices=feature_indices,\n",
    "            final_model=final_model,\n",
    "            feature_names=final_feature_names,\n",
    "            val_roc_auc=final_roc_auc,\n",
    "            best_n_features=best_config['best_n_features'],\n",
    "            best_params=best_config['best_result']['best_params'],\n",
    "            test_predictions=test_predictions\n",
    "        )\n",
    "\n",
    "# Usage function\n",
    "def optimized_feature_selection(X_train, y_train, X_val, y_val, X_test, feature_names=None,\n",
    "                                cv_folds=5, n_jobs=-1, random_state=42) -> FeatureSelectionResult:\n",
    "    \"\"\"\n",
    "    Optimized feature selection pipeline for maximum ROC AUC\n",
    "    \n",
    "    Args:\n",
    "        X_train, y_train: Training data and labels\n",
    "        X_val, y_val: Validation data and labels  \n",
    "        X_test: Test data\n",
    "        feature_names: Optional feature names\n",
    "        cv_folds: Number of CV folds for hyperparameter tuning\n",
    "        n_jobs: Number of parallel jobs (-1 for all cores)\n",
    "        random_state: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        FeatureSelectionResult with optimized model and selected features\n",
    "    \"\"\"\n",
    "    selector = OptimizedFeatureSelector(cv_folds=cv_folds, n_jobs=n_jobs, random_state=random_state)\n",
    "    return selector.fit(X_train, y_train, X_val, y_val, X_test, feature_names)\n"
   ],
   "outputs": [],
   "execution_count": 375
  },
  {
   "cell_type": "code",
   "source": [
    "results: FeatureSelectionResult = optimized_feature_selection(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_val,\n",
    "    y_val,\n",
    "    X_test,\n",
    "    X_train.columns,\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X4y2MZ-IUPBe",
    "outputId": "7312cd53-00d8-4e9a-c3d3-c2016bae4cd4",
    "ExecuteTime": {
     "end_time": "2025-06-05T01:21:38.130466Z",
     "start_time": "2025-06-05T00:29:38.081325Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 20:29:38,082 - INFO - Starting optimized feature selection pipeline...\n",
      "2025-06-04 20:29:38,121 - INFO - Testing StandardScaler...\n",
      "2025-06-04 20:29:40,164 - INFO - Testing 110 features...\n",
      "2025-06-04 20:32:14,146 - INFO - Features: 110, Val ROC AUC: 0.8247, CV Score: 0.8160\n",
      "2025-06-04 20:32:14,146 - INFO - Testing 115 features...\n",
      "2025-06-04 20:34:29,192 - INFO - Features: 115, Val ROC AUC: 0.8237, CV Score: 0.8153\n",
      "2025-06-04 20:34:29,193 - INFO - Testing 120 features...\n",
      "2025-06-04 20:36:58,010 - INFO - Features: 120, Val ROC AUC: 0.8235, CV Score: 0.8160\n",
      "2025-06-04 20:36:58,011 - INFO - Testing 125 features...\n",
      "2025-06-04 20:39:04,981 - INFO - Features: 125, Val ROC AUC: 0.8229, CV Score: 0.8161\n",
      "2025-06-04 20:39:04,982 - INFO - Testing 130 features...\n",
      "2025-06-04 20:41:19,054 - INFO - Features: 130, Val ROC AUC: 0.8236, CV Score: 0.8154\n",
      "2025-06-04 20:41:19,055 - INFO - Testing 135 features...\n",
      "2025-06-04 20:43:28,902 - INFO - Features: 135, Val ROC AUC: 0.8239, CV Score: 0.8156\n",
      "2025-06-04 20:43:28,902 - INFO - Testing 140 features...\n",
      "2025-06-04 20:45:38,837 - INFO - Features: 140, Val ROC AUC: 0.8227, CV Score: 0.8154\n",
      "2025-06-04 20:45:38,838 - INFO - Testing 145 features...\n",
      "2025-06-04 20:47:54,471 - INFO - Features: 145, Val ROC AUC: 0.8240, CV Score: 0.8156\n",
      "2025-06-04 20:47:54,472 - INFO - Testing 150 features...\n",
      "2025-06-04 20:50:28,637 - INFO - Features: 150, Val ROC AUC: 0.8241, CV Score: 0.8160\n",
      "2025-06-04 20:50:28,638 - INFO - Testing 155 features...\n",
      "2025-06-04 20:52:58,543 - INFO - Features: 155, Val ROC AUC: 0.8234, CV Score: 0.8160\n",
      "2025-06-04 20:52:58,543 - INFO - Testing 160 features...\n",
      "2025-06-04 20:55:14,010 - INFO - Features: 160, Val ROC AUC: 0.8223, CV Score: 0.8155\n",
      "2025-06-04 20:55:14,011 - INFO - Testing 165 features...\n",
      "2025-06-04 20:57:18,329 - INFO - Features: 165, Val ROC AUC: 0.8234, CV Score: 0.8155\n",
      "2025-06-04 20:57:18,330 - INFO - Testing 170 features...\n",
      "2025-06-04 20:59:47,133 - INFO - Features: 170, Val ROC AUC: 0.8242, CV Score: 0.8156\n",
      "2025-06-04 20:59:47,134 - INFO - Testing 175 features...\n",
      "2025-06-04 21:02:10,322 - INFO - Features: 175, Val ROC AUC: 0.8239, CV Score: 0.8159\n",
      "2025-06-04 21:02:10,322 - INFO - Testing 180 features...\n",
      "2025-06-04 21:04:42,592 - INFO - Features: 180, Val ROC AUC: 0.8240, CV Score: 0.8156\n",
      "2025-06-04 21:04:42,593 - INFO - Testing 185 features...\n",
      "2025-06-04 21:07:16,225 - INFO - Features: 185, Val ROC AUC: 0.8247, CV Score: 0.8158\n",
      "2025-06-04 21:07:16,226 - INFO - Testing 190 features...\n",
      "2025-06-04 21:09:18,624 - INFO - Features: 190, Val ROC AUC: 0.8236, CV Score: 0.8157\n",
      "2025-06-04 21:09:18,625 - INFO - Testing 195 features...\n",
      "2025-06-04 21:11:32,341 - INFO - Features: 195, Val ROC AUC: 0.8239, CV Score: 0.8157\n",
      "2025-06-04 21:11:32,342 - INFO - Testing 200 features...\n",
      "2025-06-04 21:13:44,534 - INFO - Features: 200, Val ROC AUC: 0.8243, CV Score: 0.8154\n",
      "2025-06-04 21:13:44,534 - INFO - Testing 205 features...\n",
      "2025-06-04 21:15:49,572 - INFO - Features: 205, Val ROC AUC: 0.8230, CV Score: 0.8159\n",
      "2025-06-04 21:15:49,573 - INFO - Testing 210 features...\n",
      "2025-06-04 21:17:49,287 - INFO - Features: 210, Val ROC AUC: 0.8236, CV Score: 0.8155\n",
      "2025-06-04 21:17:49,288 - INFO - Testing 215 features...\n",
      "2025-06-04 21:19:55,784 - INFO - Features: 215, Val ROC AUC: 0.8240, CV Score: 0.8155\n",
      "2025-06-04 21:19:55,784 - INFO - Testing 220 features...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Developer\\bu\\kaggle\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1682\u001B[39m, in \u001B[36mParallel._get_outputs\u001B[39m\u001B[34m(self, iterator, pre_dispatch)\u001B[39m\n\u001B[32m   1681\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backend.retrieval_context():\n\u001B[32m-> \u001B[39m\u001B[32m1682\u001B[39m         \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m._retrieve()\n\u001B[32m   1684\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mGeneratorExit\u001B[39;00m:\n\u001B[32m   1685\u001B[39m     \u001B[38;5;66;03m# The generator has been garbage collected before being fully\u001B[39;00m\n\u001B[32m   1686\u001B[39m     \u001B[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001B[39;00m\n\u001B[32m   1687\u001B[39m     \u001B[38;5;66;03m# the user if necessary.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Developer\\bu\\kaggle\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1800\u001B[39m, in \u001B[36mParallel._retrieve\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1797\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (nb_jobs == \u001B[32m0\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[32m   1798\u001B[39m     \u001B[38;5;28mself\u001B[39m._jobs[\u001B[32m0\u001B[39m].get_status(timeout=\u001B[38;5;28mself\u001B[39m.timeout) == TASK_PENDING\n\u001B[32m   1799\u001B[39m ):\n\u001B[32m-> \u001B[39m\u001B[32m1800\u001B[39m     \u001B[43mtime\u001B[49m\u001B[43m.\u001B[49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m   1801\u001B[39m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[376]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m results = \u001B[43moptimized_feature_selection\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      2\u001B[39m \u001B[43m    \u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      3\u001B[39m \u001B[43m    \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[43m    \u001B[49m\u001B[43mX_val\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[43m    \u001B[49m\u001B[43my_val\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[43m    \u001B[49m\u001B[43mX_test\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      7\u001B[39m \u001B[43m    \u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      8\u001B[39m \u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[375]\u001B[39m\u001B[32m, line 351\u001B[39m, in \u001B[36moptimized_feature_selection\u001B[39m\u001B[34m(X_train, y_train, X_val, y_val, X_test, feature_names, cv_folds, n_jobs, random_state)\u001B[39m\n\u001B[32m    335\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    336\u001B[39m \u001B[33;03mOptimized feature selection pipeline for maximum ROC AUC\u001B[39;00m\n\u001B[32m    337\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m    348\u001B[39m \u001B[33;03m    FeatureSelectionResult with optimized model and selected features\u001B[39;00m\n\u001B[32m    349\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    350\u001B[39m selector = OptimizedFeatureSelector(cv_folds=cv_folds, n_jobs=n_jobs, random_state=random_state)\n\u001B[32m--> \u001B[39m\u001B[32m351\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mselector\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_val\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_val\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeature_names\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[375]\u001B[39m\u001B[32m, line 273\u001B[39m, in \u001B[36mOptimizedFeatureSelector.fit\u001B[39m\u001B[34m(self, X_train, y_train, X_val, y_val, X_test, feature_names)\u001B[39m\n\u001B[32m    271\u001B[39m \u001B[38;5;66;03m# Test each scaler\u001B[39;00m\n\u001B[32m    272\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m scaler_name, scaler \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.scalers.items():\n\u001B[32m--> \u001B[39m\u001B[32m273\u001B[39m     config = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_evaluate_scaler\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    274\u001B[39m \u001B[43m        \u001B[49m\u001B[43mscaler_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscaler\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_val\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_test\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    275\u001B[39m \u001B[43m        \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_val\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeature_names\u001B[49m\n\u001B[32m    276\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    278\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m config \u001B[38;5;129;01mand\u001B[39;00m config[\u001B[33m'\u001B[39m\u001B[33mbest_result\u001B[39m\u001B[33m'\u001B[39m][\u001B[33m'\u001B[39m\u001B[33mroc_auc\u001B[39m\u001B[33m'\u001B[39m] > best_overall_score:\n\u001B[32m    279\u001B[39m         best_overall_score = config[\u001B[33m'\u001B[39m\u001B[33mbest_result\u001B[39m\u001B[33m'\u001B[39m][\u001B[33m'\u001B[39m\u001B[33mroc_auc\u001B[39m\u001B[33m'\u001B[39m]\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[375]\u001B[39m\u001B[32m, line 225\u001B[39m, in \u001B[36mOptimizedFeatureSelector._evaluate_scaler\u001B[39m\u001B[34m(self, scaler_name, scaler, X_train, X_val, X_test, y_train, y_val, feature_names)\u001B[39m\n\u001B[32m    222\u001B[39m feature_importance = \u001B[38;5;28mself\u001B[39m._get_feature_importance(X_train_scaled, y_train)\n\u001B[32m    224\u001B[39m \u001B[38;5;66;03m# Test different feature counts\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m225\u001B[39m results = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_test_feature_counts\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    226\u001B[39m \u001B[43m    \u001B[49m\u001B[43mX_train_scaled\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_val_scaled\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_val\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    227\u001B[39m \u001B[43m    \u001B[49m\u001B[43mfeature_importance\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mfeature_names\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    228\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    230\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m results:\n\u001B[32m    231\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[375]\u001B[39m\u001B[32m, line 190\u001B[39m, in \u001B[36mOptimizedFeatureSelector._test_feature_counts\u001B[39m\u001B[34m(self, X_train, X_val, y_train, y_val, feature_importance, max_features)\u001B[39m\n\u001B[32m    187\u001B[39m X_val_subset = X_val[:, top_features_idx]\n\u001B[32m    189\u001B[39m \u001B[38;5;66;03m# Optimize hyperparameters\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m190\u001B[39m grid_search = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_optimize_hyperparameters\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train_subset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    192\u001B[39m \u001B[38;5;66;03m# Evaluate on validation set\u001B[39;00m\n\u001B[32m    193\u001B[39m best_model = grid_search.best_estimator_\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[375]\u001B[39m\u001B[32m, line 164\u001B[39m, in \u001B[36mOptimizedFeatureSelector._optimize_hyperparameters\u001B[39m\u001B[34m(self, X_train, y_train)\u001B[39m\n\u001B[32m    155\u001B[39m rf = RandomForestClassifier(random_state=\u001B[38;5;28mself\u001B[39m.random_state, n_jobs=\u001B[32m1\u001B[39m)  \u001B[38;5;66;03m# n_jobs=1 for GridSearch parallelization\u001B[39;00m\n\u001B[32m    156\u001B[39m grid_search = HalvingGridSearchCV(\n\u001B[32m    157\u001B[39m     estimator=rf,\n\u001B[32m    158\u001B[39m     param_grid=\u001B[38;5;28mself\u001B[39m.rf_param_grid,\n\u001B[32m   (...)\u001B[39m\u001B[32m    162\u001B[39m     return_train_score=\u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m    163\u001B[39m )\n\u001B[32m--> \u001B[39m\u001B[32m164\u001B[39m \u001B[43mgrid_search\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    165\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m grid_search\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Developer\\bu\\kaggle\\.venv\\Lib\\site-packages\\sklearn\\base.py:1389\u001B[39m, in \u001B[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[39m\u001B[34m(estimator, *args, **kwargs)\u001B[39m\n\u001B[32m   1382\u001B[39m     estimator._validate_params()\n\u001B[32m   1384\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[32m   1385\u001B[39m     skip_parameter_validation=(\n\u001B[32m   1386\u001B[39m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[32m   1387\u001B[39m     )\n\u001B[32m   1388\u001B[39m ):\n\u001B[32m-> \u001B[39m\u001B[32m1389\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Developer\\bu\\kaggle\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search_successive_halving.py:253\u001B[39m, in \u001B[36mBaseSuccessiveHalving.fit\u001B[39m\u001B[34m(self, X, y, **params)\u001B[39m\n\u001B[32m    247\u001B[39m \u001B[38;5;28mself\u001B[39m._check_input_parameters(\n\u001B[32m    248\u001B[39m     X=X, y=y, split_params=routed_params.splitter.split\n\u001B[32m    249\u001B[39m )\n\u001B[32m    251\u001B[39m \u001B[38;5;28mself\u001B[39m._n_samples_orig = _num_samples(X)\n\u001B[32m--> \u001B[39m\u001B[32m253\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m=\u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    255\u001B[39m \u001B[38;5;66;03m# Set best_score_: BaseSearchCV does not set it, as refit is a callable\u001B[39;00m\n\u001B[32m    256\u001B[39m \u001B[38;5;28mself\u001B[39m.best_score_ = \u001B[38;5;28mself\u001B[39m.cv_results_[\u001B[33m\"\u001B[39m\u001B[33mmean_test_score\u001B[39m\u001B[33m\"\u001B[39m][\u001B[38;5;28mself\u001B[39m.best_index_]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Developer\\bu\\kaggle\\.venv\\Lib\\site-packages\\sklearn\\base.py:1389\u001B[39m, in \u001B[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[39m\u001B[34m(estimator, *args, **kwargs)\u001B[39m\n\u001B[32m   1382\u001B[39m     estimator._validate_params()\n\u001B[32m   1384\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[32m   1385\u001B[39m     skip_parameter_validation=(\n\u001B[32m   1386\u001B[39m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[32m   1387\u001B[39m     )\n\u001B[32m   1388\u001B[39m ):\n\u001B[32m-> \u001B[39m\u001B[32m1389\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Developer\\bu\\kaggle\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1024\u001B[39m, in \u001B[36mBaseSearchCV.fit\u001B[39m\u001B[34m(self, X, y, **params)\u001B[39m\n\u001B[32m   1018\u001B[39m     results = \u001B[38;5;28mself\u001B[39m._format_results(\n\u001B[32m   1019\u001B[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001B[32m   1020\u001B[39m     )\n\u001B[32m   1022\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m results\n\u001B[32m-> \u001B[39m\u001B[32m1024\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_run_search\u001B[49m\u001B[43m(\u001B[49m\u001B[43mevaluate_candidates\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1026\u001B[39m \u001B[38;5;66;03m# multimetric is determined here because in the case of a callable\u001B[39;00m\n\u001B[32m   1027\u001B[39m \u001B[38;5;66;03m# self.scoring the return type is only known after calling\u001B[39;00m\n\u001B[32m   1028\u001B[39m first_test_score = all_out[\u001B[32m0\u001B[39m][\u001B[33m\"\u001B[39m\u001B[33mtest_scores\u001B[39m\u001B[33m\"\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Developer\\bu\\kaggle\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search_successive_halving.py:357\u001B[39m, in \u001B[36mBaseSuccessiveHalving._run_search\u001B[39m\u001B[34m(self, evaluate_candidates)\u001B[39m\n\u001B[32m    350\u001B[39m     cv = \u001B[38;5;28mself\u001B[39m._checked_cv_orig\n\u001B[32m    352\u001B[39m more_results = {\n\u001B[32m    353\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33miter\u001B[39m\u001B[33m\"\u001B[39m: [itr] * n_candidates,\n\u001B[32m    354\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mn_resources\u001B[39m\u001B[33m\"\u001B[39m: [n_resources] * n_candidates,\n\u001B[32m    355\u001B[39m }\n\u001B[32m--> \u001B[39m\u001B[32m357\u001B[39m results = \u001B[43mevaluate_candidates\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    358\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcandidate_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmore_results\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmore_results\u001B[49m\n\u001B[32m    359\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    361\u001B[39m n_candidates_to_keep = ceil(n_candidates / \u001B[38;5;28mself\u001B[39m.factor)\n\u001B[32m    362\u001B[39m candidate_params = _top_k(results, n_candidates_to_keep, itr)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Developer\\bu\\kaggle\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001B[39m, in \u001B[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001B[39m\u001B[34m(candidate_params, cv, more_results)\u001B[39m\n\u001B[32m    962\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.verbose > \u001B[32m0\u001B[39m:\n\u001B[32m    963\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\n\u001B[32m    964\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mFitting \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[33m folds for each of \u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[33m candidates,\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    965\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33m totalling \u001B[39m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[33m fits\u001B[39m\u001B[33m\"\u001B[39m.format(\n\u001B[32m    966\u001B[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001B[32m    967\u001B[39m         )\n\u001B[32m    968\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m970\u001B[39m out = \u001B[43mparallel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    971\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_fit_and_score\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    972\u001B[39m \u001B[43m        \u001B[49m\u001B[43mclone\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase_estimator\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    973\u001B[39m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    974\u001B[39m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    975\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    976\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    977\u001B[39m \u001B[43m        \u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m=\u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    978\u001B[39m \u001B[43m        \u001B[49m\u001B[43msplit_progress\u001B[49m\u001B[43m=\u001B[49m\u001B[43m(\u001B[49m\u001B[43msplit_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_splits\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    979\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcandidate_progress\u001B[49m\u001B[43m=\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcand_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_candidates\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    980\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mfit_and_score_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    981\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    982\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mcand_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43msplit_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mproduct\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    983\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcandidate_params\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    984\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcv\u001B[49m\u001B[43m.\u001B[49m\u001B[43msplit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mrouted_params\u001B[49m\u001B[43m.\u001B[49m\u001B[43msplitter\u001B[49m\u001B[43m.\u001B[49m\u001B[43msplit\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    985\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    986\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    988\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(out) < \u001B[32m1\u001B[39m:\n\u001B[32m    989\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    990\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mNo fits were performed. \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    991\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mWas the CV iterator empty? \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    992\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mWere there no candidates?\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    993\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Developer\\bu\\kaggle\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001B[39m, in \u001B[36mParallel.__call__\u001B[39m\u001B[34m(self, iterable)\u001B[39m\n\u001B[32m     72\u001B[39m config = get_config()\n\u001B[32m     73\u001B[39m iterable_with_config = (\n\u001B[32m     74\u001B[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[32m     75\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[32m     76\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m77\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterable_with_config\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Developer\\bu\\kaggle\\.venv\\Lib\\site-packages\\joblib\\parallel.py:2072\u001B[39m, in \u001B[36mParallel.__call__\u001B[39m\u001B[34m(self, iterable)\u001B[39m\n\u001B[32m   2066\u001B[39m \u001B[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001B[39;00m\n\u001B[32m   2067\u001B[39m \u001B[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001B[39;00m\n\u001B[32m   2068\u001B[39m \u001B[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001B[39;00m\n\u001B[32m   2069\u001B[39m \u001B[38;5;66;03m# dispatch of the tasks to the workers.\u001B[39;00m\n\u001B[32m   2070\u001B[39m \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[32m-> \u001B[39m\u001B[32m2072\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.return_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Developer\\bu\\kaggle\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1732\u001B[39m, in \u001B[36mParallel._get_outputs\u001B[39m\u001B[34m(self, iterator, pre_dispatch)\u001B[39m\n\u001B[32m   1730\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m:\n\u001B[32m   1731\u001B[39m     \u001B[38;5;28mself\u001B[39m._exception = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1732\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_abort\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1733\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[32m   1734\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m   1735\u001B[39m     \u001B[38;5;66;03m# Store the unconsumed tasks and terminate the workers if necessary\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Developer\\bu\\kaggle\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1646\u001B[39m, in \u001B[36mParallel._abort\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1641\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m._aborted \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(backend, \u001B[33m\"\u001B[39m\u001B[33mabort_everything\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m   1642\u001B[39m     \u001B[38;5;66;03m# If the backend is managed externally we need to make sure\u001B[39;00m\n\u001B[32m   1643\u001B[39m     \u001B[38;5;66;03m# to leave it in a working state to allow for future jobs\u001B[39;00m\n\u001B[32m   1644\u001B[39m     \u001B[38;5;66;03m# scheduling.\u001B[39;00m\n\u001B[32m   1645\u001B[39m     ensure_ready = \u001B[38;5;28mself\u001B[39m._managed_backend\n\u001B[32m-> \u001B[39m\u001B[32m1646\u001B[39m     \u001B[43mbackend\u001B[49m\u001B[43m.\u001B[49m\u001B[43mabort_everything\u001B[49m\u001B[43m(\u001B[49m\u001B[43mensure_ready\u001B[49m\u001B[43m=\u001B[49m\u001B[43mensure_ready\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1647\u001B[39m \u001B[38;5;28mself\u001B[39m._aborted = \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Developer\\bu\\kaggle\\.venv\\Lib\\site-packages\\joblib\\_parallel_backends.py:725\u001B[39m, in \u001B[36mLokyBackend.abort_everything\u001B[39m\u001B[34m(self, ensure_ready)\u001B[39m\n\u001B[32m    723\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mabort_everything\u001B[39m(\u001B[38;5;28mself\u001B[39m, ensure_ready=\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[32m    724\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Shutdown the workers and restart a new one with the same parameters\"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m725\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_workers\u001B[49m\u001B[43m.\u001B[49m\u001B[43mterminate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkill_workers\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m    726\u001B[39m     \u001B[38;5;28mself\u001B[39m._workers = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    728\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m ensure_ready:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Developer\\bu\\kaggle\\.venv\\Lib\\site-packages\\joblib\\executor.py:86\u001B[39m, in \u001B[36mMemmappingExecutor.terminate\u001B[39m\u001B[34m(self, kill_workers)\u001B[39m\n\u001B[32m     85\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mterminate\u001B[39m(\u001B[38;5;28mself\u001B[39m, kill_workers=\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[32m---> \u001B[39m\u001B[32m86\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mshutdown\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkill_workers\u001B[49m\u001B[43m=\u001B[49m\u001B[43mkill_workers\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     88\u001B[39m     \u001B[38;5;66;03m# When workers are killed in a brutal manner, they cannot execute the\u001B[39;00m\n\u001B[32m     89\u001B[39m     \u001B[38;5;66;03m# finalizer of their shared memmaps. The refcount of those memmaps may\u001B[39;00m\n\u001B[32m     90\u001B[39m     \u001B[38;5;66;03m# be off by an unknown number, so instead of decref'ing them, we force\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m     95\u001B[39m     \u001B[38;5;66;03m# with allow_non_empty=True but if we can't, it will be clean up later\u001B[39;00m\n\u001B[32m     96\u001B[39m     \u001B[38;5;66;03m# on by the resource_tracker.\u001B[39;00m\n\u001B[32m     97\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m._submit_resize_lock:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Developer\\bu\\kaggle\\.venv\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:1333\u001B[39m, in \u001B[36mProcessPoolExecutor.shutdown\u001B[39m\u001B[34m(self, wait, kill_workers)\u001B[39m\n\u001B[32m   1329\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m executor_manager_thread \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m wait:\n\u001B[32m   1330\u001B[39m     \u001B[38;5;66;03m# This locks avoids concurrent join if the interpreter\u001B[39;00m\n\u001B[32m   1331\u001B[39m     \u001B[38;5;66;03m# is shutting down.\u001B[39;00m\n\u001B[32m   1332\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m _global_shutdown_lock:\n\u001B[32m-> \u001B[39m\u001B[32m1333\u001B[39m         \u001B[43mexecutor_manager_thread\u001B[49m\u001B[43m.\u001B[49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1334\u001B[39m         _threads_wakeups.pop(executor_manager_thread, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[32m   1336\u001B[39m \u001B[38;5;66;03m# To reduce the risk of opening too many files, remove references to\u001B[39;00m\n\u001B[32m   1337\u001B[39m \u001B[38;5;66;03m# objects that use file descriptors.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py:1147\u001B[39m, in \u001B[36mThread.join\u001B[39m\u001B[34m(self, timeout)\u001B[39m\n\u001B[32m   1144\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mcannot join current thread\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m   1146\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1147\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_wait_for_tstate_lock\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1148\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1149\u001B[39m     \u001B[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001B[39;00m\n\u001B[32m   1150\u001B[39m     \u001B[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001B[39;00m\n\u001B[32m   1151\u001B[39m     \u001B[38;5;28mself\u001B[39m._wait_for_tstate_lock(timeout=\u001B[38;5;28mmax\u001B[39m(timeout, \u001B[32m0\u001B[39m))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py:1167\u001B[39m, in \u001B[36mThread._wait_for_tstate_lock\u001B[39m\u001B[34m(self, block, timeout)\u001B[39m\n\u001B[32m   1164\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[32m   1166\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1167\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mlock\u001B[49m\u001B[43m.\u001B[49m\u001B[43macquire\u001B[49m\u001B[43m(\u001B[49m\u001B[43mblock\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[32m   1168\u001B[39m         lock.release()\n\u001B[32m   1169\u001B[39m         \u001B[38;5;28mself\u001B[39m._stop()\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 376
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "y_test_pred = results.test_predictions\n",
    "y_test_pred\n",
    "\n",
    "# \"\"\"Data class to hold feature selection results\"\"\"\n",
    "# best_scaler_name: str\n",
    "# best_scaler: Any\n",
    "# feature_indices: np.ndarray\n",
    "# final_model: RandomForestClassifier\n",
    "# feature_names: np.ndarray\n",
    "# val_roc_auc: float\n",
    "# best_n_features: int\n",
    "# best_params: Dict\n",
    "# test_predictions: np.ndarray\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "submission_array = np.vstack([test_features['ID'].values, y_test_pred]).T\n",
    "submission_df = pd.DataFrame(submission_array, columns=['ID', 'radiant_win']).reset_index(drop=True)\n",
    "submission_df.to_csv('simple_baseline.csv', index=None)"
   ],
   "metadata": {
    "id": "0ZlqtdYdUALp"
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
